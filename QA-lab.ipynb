{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a6d061-2a42-4cb6-867d-c2883b212414",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search\n",
    "\n",
    "GPT excels at answering questions, but only on topics it remembers from its training data.  In this lab, the seach-ask method is used to answer questions using private data.\n",
    "\n",
    "1. Search: search your library of text for relevant text sections.\n",
    "2. Ask: insert the retrieved text sections into a message to ChatGPT and ask it the question\n",
    "\n",
    "To complete this lab, you will need to set up an OpenAI API account and set up your development enviroment. Here's a link to the [QuickStart](https://platform.openai.com/docs/quickstart?context=python). Pay special attention to step 2 - set up your API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdcc47-cf26-4d84-bfea-398b50d30a53",
   "metadata": {},
   "source": [
    "## Load Text to Search\n",
    "\n",
    "Place the documents that your application will search in the docs folder.  \n",
    "\n",
    "This functions read various types of text files (PDF, DOCX, TXT).  The following libraries are used:\n",
    "- [PyPDF](https://pypdf.readthedocs.io/en/stable/)\n",
    "- [python-docx](https://python-docx.readthedocs.io/en/latest/)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0c102-2f64-4be3-aee7-2e6960ce0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-docx\n",
    "#!pip install pypdf\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import docx \n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            combined_text += read_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            combined_text += read_word(file_path)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    return combined_text\n",
    "\n",
    "train_directory = 'docs/'\n",
    "text = read_documents_from_directory(train_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b80b99-88a5-4bb5-85bd-cc455a52e1aa",
   "metadata": {},
   "source": [
    "## Split into chunks\n",
    "\n",
    "This lab uses the [LangChain](https://www.langchain.com) library. **LangChain** is a framework for developing applications powered by large language models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229ff58-2a8e-4bc8-a3a0-86f700c80300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "char_text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, \n",
    "                                      chunk_overlap=200, length_function=len)\n",
    "\n",
    "text_chunks = char_text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9912c2-41de-4d8e-b41e-c0ca2b2c72d4",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "There are two steps for create an easily searchable embedding.  \n",
    "\n",
    "First create an [OpenAI embedding](https://platform.openai.com/docs/guides/embeddings) using the LangChain library.  Then create a [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) vector database for efficient search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa5ff7-a812-4804-8668-4e8865bee327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai\n",
    "#!pip install langchain_community\n",
    "#!pip install faiss-cpu #or !pip instal faiss-gpu\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = FAISS.from_texts(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a471c7-1861-4490-9db3-577fb652825c",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Write five questions related to your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b623cf-7ae8-4b30-9b10-e7670a100b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Do emeritus professors have library privileges?\", \n",
    "    \"How many years of service are required to be eligible for emeritus status?\", \n",
    "    \"Who are the main characters in Emma?\", \n",
    "    \"When are Dr. Howard's office hours?\", \n",
    "    \"For Natural Language Processing, what percentage of the final grade are homework assignments?\", \n",
    "    \"When and where does Mobile Application Development meet?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98ac1f-bd32-4c03-8ac4-db3fc21a07c1",
   "metadata": {},
   "source": [
    "Create a LangChain chain to send queries and related text to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1ed6f-43e3-4ebe-9a67-c9e235689a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()  #get a reference to the LLM\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\") #create a LangChain chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1305e5b-aba8-494b-8628-ea7cf8a83b28",
   "metadata": {},
   "source": [
    "## Answer Questions\n",
    "\n",
    "For each question\n",
    "1. Search the embedding for related text\n",
    "2. Send the related text and the question to the OpenAI api\n",
    "3. Print thte question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cd492-a47f-4882-8702-c96047acadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    docs = docsearch.similarity_search(query )\n",
    "    response = chain.invoke({\"input_documents\" : docs, \"question\" :query})\n",
    "    print(\" \")\n",
    "    print(query)\n",
    "    print(response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae880b7-7fc7-4495-aa91-76e155c9b85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
